import sys
if hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding="utf-8")

import os
import time
import mimetypes
from typing import List, Tuple, Any

from dotenv import load_dotenv
load_dotenv()

# === Azure AI Inference (GitHub Models endpoint) ===
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import (
    SystemMessage, UserMessage, AssistantMessage,
    TextContentItem, ImageContentItem, ImageUrl
)
from azure.core.credentials import AzureKeyCredential

# === Minimal file parsers ===
from PyPDF2 import PdfReader
from PIL import Image

import gradio as gr

# -------- Config --------
ENDPOINT = os.getenv("ENDPOINT", "https://models.github.ai/inference")
MODEL = os.getenv("MODEL", "openai/gpt-4.1")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

client = ChatCompletionsClient(
    endpoint=ENDPOINT,
    credential=AzureKeyCredential(GITHUB_TOKEN),
)

# === Token usage (session accumulators) ===
SESSION_PROMPT_TOKENS = 0
SESSION_COMPLETION_TOKENS = 0
SESSION_TOTAL_TOKENS = 0

# -------- Helpers --------
IMAGE_EXT_MAP = {
    ".jpg": "jpeg",
    ".jpeg": "jpeg",
    ".png": "png",
    ".webp": "webp",
    ".bmp": "bmp",
}

TEXT_LIKE = {".txt", ".md", ".csv", ".log"}
PDF_LIKE = {".pdf"}

def is_image(path: str) -> bool:
    mt, _ = mimetypes.guess_type(path)
    return (mt or "").startswith("image/")

def read_text_from_file(path: str, max_chars: int = 20000) -> str:
    _, ext = os.path.splitext(path.lower())
    if ext in TEXT_LIKE:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read()[:max_chars]

    if ext in PDF_LIKE:
        text = []
        try:
            reader = PdfReader(path)
            for page in reader.pages:
                text.append(page.extract_text() or "")
        except Exception as e:
            return f"[PDF read error: {e}]"
        return ("\n".join(text))[:max_chars]

    # ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏ô‡∏¥‡∏î‡∏≠‡∏∑‡πà‡∏ô‡∏ï‡∏£‡∏á‡πÜ: ‡πÅ‡∏ô‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏â‡∏¢‡πÜ
    return f"[Unsupported file type: {os.path.basename(path)}]"

def image_to_content_item(path: str) -> ImageContentItem:
    _, ext = os.path.splitext(path.lower())
    img_fmt = IMAGE_EXT_MAP.get(ext, "png")  # default png
    img_url = ImageUrl.load(image_file=path, image_format=img_fmt)
    return ImageContentItem(image_url=img_url)

def to_text_item(text: str) -> TextContentItem:
    return TextContentItem(text=text)

def history_to_messages(history: List[Any]) -> List[Any]:
    """
    ‡πÅ‡∏õ‡∏•‡∏á history ‡∏à‡∏≤‡∏Å gr.ChatInterface ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Azure
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö [(user, assistant), ...] ‡πÅ‡∏•‡∏∞/‡∏´‡∏£‡∏∑‡∏≠ ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dict messages
    """
    msgs = []
    if not history:
        return msgs

    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤ 8 ‡πÄ‡∏ó‡∏¥‡∏£‡πå‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏û‡∏≠ (‡∏Å‡∏±‡∏ô prompt ‡πÇ‡∏ï‡πÄ‡∏Å‡∏¥‡∏ô)
    trimmed = history[-8:]

    for turn in trimmed:
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö tuple: (user_text, assistant_text)
        if isinstance(turn, (list, tuple)) and len(turn) == 2:
            u, a = turn[0] or "", turn[1] or ""
            if isinstance(u, str) and u.strip():
                msgs.append(UserMessage(content=[to_text_item(u)]))
            elif isinstance(u, dict) and "text" in u:
                text_u = u.get("text") or ""
                msgs.append(UserMessage(content=[to_text_item(text_u)]))

            if isinstance(a, str) and a.strip():
                msgs.append(AssistantMessage(content=[to_text_item(a)]))

        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dict ‡∏ï‡∏≤‡∏° type="messages"
        elif isinstance(turn, dict) and "role" in turn and "content" in turn:
            role = turn["role"]
            content = turn["content"]
            text_join = ""
            if isinstance(content, list):
                text_join = " ".join([c.get("text", "") for c in content if isinstance(c, dict)])
            elif isinstance(content, str):
                text_join = content
            if role == "user":
                msgs.append(UserMessage(content=[to_text_item(text_join)]))
            elif role == "assistant":
                msgs.append(AssistantMessage(content=[to_text_item(text_join)]))

    return msgs

# -------- Core chat function for Gradio --------
def chat_fn(message, history, system_prompt, model_name, max_tokens, temperature):
    """
    message: dict {"text": str, "files": [tmp_paths]}
    history: previous chat display (list)
    """
    global SESSION_PROMPT_TOKENS, SESSION_COMPLETION_TOKENS, SESSION_TOTAL_TOKENS

    t0 = time.perf_counter()

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏ö
    messages = []
    if system_prompt and system_prompt.strip():
        messages.append(SystemMessage(content=system_prompt.strip()))

    # ‡∏ô‡∏≥ history ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏ß‡∏ô)
    messages.extend(history_to_messages(history))

    # ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å message (image & docs)
    user_text = (message or {}).get("text") or ""
    files = (message or {}).get("files") or []

    # ‡πÅ‡∏¢‡∏Å image ‡∏Å‡∏±‡∏ö non-image
    image_files = [f for f in files if is_image(f)]
    other_files = [f for f in files if not is_image(f)]

    user_contents = []
    if user_text.strip():
        user_contents.append(to_text_item(user_text.strip()))

    # ‡πÅ‡∏ô‡∏ö‡∏£‡∏π‡∏õ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ó‡∏ô‡∏ï‡πå (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ)
    for path in image_files:
        try:
            user_contents.append(image_to_content_item(path))
        except Exception as e:
            user_contents.append(to_text_item(f"[Image load error: {os.path.basename(path)}: {e}]"))

    # Extract ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏£‡∏π‡∏õ (txt/pdf ‡∏Ø‡∏•‡∏Ø)
    for path in other_files:
        snippet = read_text_from_file(path, max_chars=12000)
        header = f"\n--- file: {os.path.basename(path)} ---\n"
        user_contents.append(to_text_item(header + snippet))

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢ ‡∏Å‡πá‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÑ‡∏õ (‡∏Å‡∏±‡∏ô error)
    if not user_contents:
        user_contents = [to_text_item("")]

    messages.append(UserMessage(content=user_contents))

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    resp = client.complete(
        messages=messages,
        model=model_name or MODEL,
        max_tokens=int(max_tokens),
        temperature=float(temperature),
    )

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ content ‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á content items)
    msg = resp.choices[0].message
    content = msg.content
    if isinstance(content, list):
        out = "".join(getattr(c, "text", "") for c in content)
    else:
        out = str(content)

    latency_ms = (time.perf_counter() - t0) * 1000

    # === Token usage ===
    u = getattr(resp, "usage", None)
    per_prompt = per_completion = per_total = None
    tps = None
    if u:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SDK ‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥‡∏à‡∏∞‡∏°‡∏µ usage: prompt_tokens, completion_tokens, total_tokens
        per_prompt = getattr(u, "prompt_tokens", None)
        per_completion = getattr(u, "completion_tokens", None)
        per_total = getattr(u, "total_tokens", None)

        if isinstance(per_prompt, int):
            SESSION_PROMPT_TOKENS += per_prompt
        if isinstance(per_completion, int):
            SESSION_COMPLETION_TOKENS += per_completion
        if isinstance(per_total, int):
            SESSION_TOTAL_TOKENS += per_total

        if per_total and latency_ms > 0:
            tps = per_total / (latency_ms / 1000.0)

    # ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏±‡πâ‡∏ô ‡πÜ
    report_lines = [f"(latency: {latency_ms:.0f} ms)"]
    if per_total is not None:
        report_lines.append(
            f"tokens ‚Äî prompt:{per_prompt} | completion:{per_completion} | total:{per_total}"
        )
        report_lines.append(
            f"session ‚Äî prompt:{SESSION_PROMPT_TOKENS} | completion:{SESSION_COMPLETION_TOKENS} | total:{SESSION_TOTAL_TOKENS}"
        )
        if tps is not None:
            report_lines.append(f"throughput: {tps:.2f} tok/s")
    else:
        report_lines.append("tokens ‚Äî (no usage returned by API)")

    out += "\n\n---\n" + " | ".join(report_lines)

    return out

# -------- UI --------
with gr.Blocks(theme="soft") as demo:
    gr.Markdown(
        "## üìé Multimodal Chatbot (Gradio + Azure AI Inference)\n"
        "- ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ñ‡∏≤‡∏° + ‡πÅ‡∏ô‡∏ö‡∏£‡∏π‡∏õ/‡πÑ‡∏ü‡∏•‡πå ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï\n"
        "- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: `openai/gpt-4.1`\n"
    )

    chat = gr.ChatInterface(
        fn=chat_fn,
        multimodal=True,  # ‡∏™‡πà‡∏á text+files ‡πÑ‡∏î‡πâ
        title="Multimodal ChatBot",
        description="‡πÅ‡∏ô‡∏ö‡∏£‡∏π‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°",
        additional_inputs=[
            gr.Textbox(value="You are a helpful assistant.", label="System Prompt"),
            gr.Textbox(value=MODEL, label="Model"),
            gr.Slider(64, 4096, value=512, step=64, label="max_tokens"),
            gr.Slider(0.0, 1.0, value=0.2, step=0.1, label="temperature"),
        ],
    )

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7860, show_api=False)
